
## Kevin

- Review python interrupt callback (pending PR)

- train_local and source("train.R") will use a stable jobs directory
- convention based local/jobs and local/data for local artifacts

- rename to "train_local" and "train_cloudml"

- train_cloudml chooses a randome remote job dir, is always async, and 
  returns a job object (ID, directory, URLs). Job object should have a 
  nice print method for users that don't keep it.
  
- Job functions should take a job object or a job id (in case we have to
  reconsturct the object from the ID)
  
- job_collect is used to synchronously poll/wait for a job and to 
  write it's model into a local directory, by default "cloudml/jobs"

- Rename "gcloud" config to "cloudml"

- How exactly do I use tf learn experiment to call predict on this model. I think
  we just reconsturct the Experiment with the job/model dir and call predict on it

- Job status/control functions

- How to handle the project directory for packaging (copy, symlink, cleanup, etc.)
    - Consider copying to temporary directory and mutating + uploading that bundle

- Pull out `hyperparameters` from config and write a new YAML file with:
    trainingInput:
       hyperparmeters:
  Then pass that to gcloud with --config hyperparmeters.yml
  
  Then in our app.R generated file where we set the config pacakge hook we 
  propagate the command line options/hyperparamters back into the config list


## Cloud NEXT

- Sort out all of the gcloud sdk installation / authentication requirements
  for people other than us to use the package and document this well.

- Documentation on using the package


## Future

- Online prediction

- Satisfying R package dependencies of train.R (packrat manifest + .Rprofile?)

- Package skeleton function w/ config.yml, train.R, etc.

- Build pane integration: Custom project type exposing various commands

- rstudioapi package making available a version of system that pumps events




